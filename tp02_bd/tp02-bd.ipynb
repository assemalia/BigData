{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12756267,"sourceType":"datasetVersion","datasetId":8064124}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T17:58:31.658105Z","iopub.execute_input":"2025-10-28T17:58:31.658424Z","iopub.status.idle":"2025-10-28T17:58:33.566193Z","shell.execute_reply.started":"2025-10-28T17:58:31.658391Z","shell.execute_reply":"2025-10-28T17:58:33.565099Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/b-tree-dataset/dataset_1000000000.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Imports and utilities ---\nimport os, time, gzip, shutil\nimport pandas as pd\nimport psutil\nimport multiprocessing\nfrom pathlib import Path\n\n# Optional libs (install if needed)\n# pip install dask[complete] polars pyarrow duckdb memory-profiler\n\nimport dask.dataframe as dd\nimport polars as pl\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport duckdb\n\n# File path (the one you gave)\nCSV_PATH = \"/kaggle/input/b-tree-dataset/dataset_1000000000.csv\"\nOUT_DIR = \"./output_bigdata\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndef file_size_mb(path): \n    return os.path.getsize(path)/(1024*1024)\n\nprint(\"File exists:\", os.path.exists(CSV_PATH))\nprint(\"CSV size (MB):\", round(file_size_mb(CSV_PATH),2))\nprint(\"CPU cores:\", multiprocessing.cpu_count())\nproc = psutil.Process(os.getpid())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:00:51.223369Z","iopub.execute_input":"2025-10-28T18:00:51.223725Z","iopub.status.idle":"2025-10-28T18:00:58.315781Z","shell.execute_reply.started":"2025-10-28T18:00:51.223700Z","shell.execute_reply":"2025-10-28T18:00:58.314517Z"}},"outputs":[{"name":"stdout","text":"File exists: True\nCSV size (MB): 13722.46\nCPU cores: 4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"=14 G","metadata":{}},{"cell_type":"code","source":"# --- 1) Smart dtype inference on a small sample ---\nSAMPLE_ROWS = 200000  # adjust: small sample to infer types\nsample = pd.read_csv(CSV_PATH, nrows=SAMPLE_ROWS)\nprint(\"Sample shape:\", sample.shape)\n\n# Infer dtypes with pandas -> map to efficient dtypes\ndef optimize_dtypes_from_df(df):\n    dtype_map = {}\n    for col in df.columns:\n        ser = df[col]\n        if pd.api.types.is_integer_dtype(ser) or pd.api.types.is_integer_dtype(ser.dropna()):\n            # choose smallest integer that fits\n            dtype_map[col] = 'Int64'\n        elif pd.api.types.is_float_dtype(ser):\n            dtype_map[col] = 'float32'\n        elif pd.api.types.is_bool_dtype(ser):\n            dtype_map[col] = 'bool'\n        else:\n            # for object columns, check unique ratio and length\n            if ser.nunique() / len(ser) < 0.5 and ser.nunique() < 100000:\n                dtype_map[col] = 'category'\n            else:\n                dtype_map[col] = 'string'  # pandas nullable string\n    return dtype_map\n\ndtype_map = optimize_dtypes_from_df(sample)\nprint(\"Inferred dtype map (preview):\")\nfor k,v in list(dtype_map.items())[:10]:\n    print(k, \"->\", v)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:01:46.541031Z","iopub.execute_input":"2025-10-28T18:01:46.541667Z","iopub.status.idle":"2025-10-28T18:01:46.817388Z","shell.execute_reply.started":"2025-10-28T18:01:46.541639Z","shell.execute_reply":"2025-10-28T18:01:46.816202Z"}},"outputs":[{"name":"stdout","text":"Sample shape: (200000, 2)\nInferred dtype map (preview):\nnumeric_column -> Int64\nalphabet_column -> string\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Convert dtype_map to pandas-friendly dtypes for read_csv\npandas_dtypes = {}\nfor col, kind in dtype_map.items():\n    if kind == 'Int64':\n        pandas_dtypes[col] = 'Int64'  # pandas nullable int\n    elif kind == 'string':\n        pandas_dtypes[col] = 'string'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:02:54.222337Z","iopub.execute_input":"2025-10-28T18:02:54.222840Z","iopub.status.idle":"2025-10-28T18:02:54.230339Z","shell.execute_reply.started":"2025-10-28T18:02:54.222803Z","shell.execute_reply":"2025-10-28T18:02:54.228946Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def method_pandas_chunks(path, dtypes, chunksize=200_000, max_rows=None):\n    proc = psutil.Process(os.getpid())\n    start_mem = proc.memory_info().rss/(1024*1024)\n    start = time.time()\n    total = 0\n    peak_mem = start_mem\n    for chunk in pd.read_csv(path, dtype=dtypes, chunksize=chunksize, low_memory=False, on_bad_lines='skip'):\n        total += len(chunk)\n        mem = proc.memory_info().rss/(1024*1024)\n        if mem > peak_mem: peak_mem = mem\n        if max_rows and total >= max_rows: break\n    elapsed = time.time() - start\n    return {\"method\":\"pandas_chunks\", \"time_s\":elapsed, \"peak_ram_mb\":peak_mem, \"rows\":total}\n\nres_a = method_pandas_chunks(CSV_PATH, pandas_dtypes, chunksize=200_000)\nprint(res_a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:24.081966Z","iopub.execute_input":"2025-10-28T18:03:24.082452Z","iopub.status.idle":"2025-10-28T18:34:05.715696Z","shell.execute_reply.started":"2025-10-28T18:03:24.082415Z","shell.execute_reply":"2025-10-28T18:34:05.713517Z"}},"outputs":[{"name":"stdout","text":"{'method': 'pandas_chunks', 'time_s': 1841.608158826828, 'peak_ram_mb': 353.90625, 'rows': 1000000000}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# -------------------------\n# Method B: Dask\n# -------------------------\ndef method_dask(path, dtypes=None, blocksize=\"64MB\"):\n    proc = psutil.Process(os.getpid())\n    start = time.time()\n    # Dask can accept dtype mapping; if some dtypes incompatible, handle exceptions\n    ddf = dd.read_csv(path, dtype=dtypes, blocksize=blocksize, assume_missing=True, on_bad_lines='skip')\n    # Trigger computation (len)\n    total = ddf.shape[0].compute()\n    elapsed = time.time() - start\n    mem = proc.memory_info().rss/(1024*1024)\n    return {\"method\":\"dask\", \"time_s\":elapsed, \"peak_ram_mb\":mem, \"rows\":int(total)}\n\nres_b = method_dask(CSV_PATH, dtypes=None, blocksize=\"64MB\")\nprint(res_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:55:21.525491Z","iopub.execute_input":"2025-10-28T18:55:21.528788Z","iopub.status.idle":"2025-10-28T19:07:14.477056Z","shell.execute_reply.started":"2025-10-28T18:55:21.528683Z","shell.execute_reply":"2025-10-28T19:07:14.475726Z"}},"outputs":[{"name":"stdout","text":"{'method': 'dask', 'time_s': 712.9201920032501, 'peak_ram_mb': 1082.26953125, 'rows': 1000000000}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# -------------------------\n# Method c: compression, compress file                                                                                                                                    \nimport gzip, shutil, os, time, psutil, pandas as pd\n\n# مسار الملف الأصلي والمضغوط\nCSV_PATH = \"/kaggle/input/b-tree-dataset/dataset_1000000000.csv\"\nGZ_PATH = \"/kaggle/working/dataset_1000000000.csv.gz\"   # مجلد قابل للكتابة\n\n# دالة لحساب الحجم بالميغابايت\ndef file_size_mb(path):\n    return os.path.getsize(path) / 1024**2\n\n# دالة لحساب الذاكرة الحالية\ndef memory_usage_mb():\n    return psutil.Process(os.getpid()).memory_info().rss / 1024**2\n\n# ---------------------------\n#  ضغط الملف \n# ---------------------------\nstart = time.time()\nwith open(CSV_PATH, 'rb') as f_in, gzip.open(GZ_PATH, 'wb') as f_out:\n    shutil.copyfileobj(f_in, f_out)\nelapsed = time.time() - start\n\nprint(f\" Compression completed in {elapsed/60:.2f} min\")\nprint(f\"Compressed file size: {file_size_mb(GZ_PATH):.1f} MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T20:26:17.907237Z","iopub.execute_input":"2025-10-28T20:26:17.908994Z","execution_failed":"2025-10-28T21:28:36.762Z"}},"outputs":[{"name":"stdout","text":" Compression completed in 41.08 min\nCompressed file size: 8592.6 MB\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import gzip, shutil, os, time, psutil, pandas as pd\n\n# مسار الملف الأصلي والمضغوط\nCSV_PATH = \"/kaggle/input/b-tree-dataset/dataset_1000000000.csv\"\nGZ_PATH = \"/kaggle/working/dataset_1000000000.csv.gz\"   # مجلد قابل للكتابة\n# دالة لحساب الحجم بالميغابايت\ndef file_size_mb(path):\n    return os.path.getsize(path) / 1024**2\n\n# دالة لحساب الذاكرة الحالية\ndef memory_usage_mb():\n    return psutil.Process(os.getpid()).memory_info().rss / 1024**2\nstart = time.time()\ndf = pd.read_csv(GZ_PATH, compression='gzip', nrows=1_000_000)\nelapsed = time.time() - start\nmem = memory_usage_mb()\n\nprint(f\"Pandas (gzip): {elapsed:.2f}s | RAM={mem:.1f}MB | Sample=1M rows\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T21:38:52.963631Z","iopub.execute_input":"2025-10-28T21:38:52.964231Z","iopub.status.idle":"2025-10-28T21:38:55.693275Z","shell.execute_reply.started":"2025-10-28T21:38:52.964181Z","shell.execute_reply":"2025-10-28T21:38:55.691999Z"}},"outputs":[{"name":"stdout","text":"Pandas (gzip): 1.11s | RAM=257.3MB | Sample=1M rows\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\nresults = [\n    {'method': 'pandas_gzip', 'time_s': 1.11, 'peak_ram_mb': 257.3, 'rows': 1_000_000},\n    {'method': 'pandas_chunks', 'time_s': 1841.61, 'peak_ram_mb': 353.9, 'rows': 1_000_000_000},\n    {'method': 'dask', 'time_s': 712.92, 'peak_ram_mb': 1082.27, 'rows': 1_000_000_000},\n    {'method': 'gzip_compression', 'time_s': 41.08*60, 'peak_ram_mb': None, 'rows': 1_000_000_000},\n]\n\ndf_results = pd.DataFrame(results)\ndf_results['time_min'] = df_results['time_s'] / 60\ndf_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T21:43:25.601483Z","iopub.execute_input":"2025-10-28T21:43:25.602698Z","iopub.status.idle":"2025-10-28T21:43:25.662814Z","shell.execute_reply.started":"2025-10-28T21:43:25.602641Z","shell.execute_reply":"2025-10-28T21:43:25.661087Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"             method   time_s  peak_ram_mb        rows  time_min\n0       pandas_gzip     1.11       257.30     1000000    0.0185\n1     pandas_chunks  1841.61       353.90  1000000000   30.6935\n2              dask   712.92      1082.27  1000000000   11.8820\n3  gzip_compression  2464.80          NaN  1000000000   41.0800","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>method</th>\n      <th>time_s</th>\n      <th>peak_ram_mb</th>\n      <th>rows</th>\n      <th>time_min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pandas_gzip</td>\n      <td>1.11</td>\n      <td>257.30</td>\n      <td>1000000</td>\n      <td>0.0185</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pandas_chunks</td>\n      <td>1841.61</td>\n      <td>353.90</td>\n      <td>1000000000</td>\n      <td>30.6935</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dask</td>\n      <td>712.92</td>\n      <td>1082.27</td>\n      <td>1000000000</td>\n      <td>11.8820</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gzip_compression</td>\n      <td>2464.80</td>\n      <td>NaN</td>\n      <td>1000000000</td>\n      <td>41.0800</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"عند مقارنة تقنيات التحميل والضغط    \nتبيّن أن مكتبة   Dask  هي الأسرع في معالجة ملفات ضخمة (1 مليار صف)،\n بينما Pandas مع chunking أكثر استقرارًا وأقل استهلاكًا للذاكرة.\nأما الضغط باستخدام gzip فقلل حجم الملف إلى 8.6GB لكنه استغرق أكثر من 40 دقيقة.","metadata":{}}]}